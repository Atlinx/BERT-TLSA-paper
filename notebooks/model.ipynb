{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwdIxdbsvqd8"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Code\n",
    "\n",
    "Common functions for training and fine-tuning the BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT module_path:  c:\\Users\\Alan\\Desktop\\Open_Source\\BERT-TLSA-paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(\"INIT module_path: \", module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "DATA_DIR = module_path + \"/data\"\n",
    "MODEL_DIR = module_path + \"/model\"\n",
    "\n",
    "for data_dir in [DATA_DIR, MODEL_DIR]:\n",
    "    os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, BertModel, BertConfig\n",
    "import torch\n",
    "import enum\n",
    "from typing import cast\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class BERTSentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert: BertModel):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert.config.hidden_size, 1), # Convert from the hidden state to a single output\n",
    "            torch.nn.Sigmoid() # Constrain output between 0 and 1\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, expected_score: torch.FloatTensor, *args, **kwargs) -> dict:\n",
    "        bert_output = self.bert(*args, **kwargs)\n",
    "        # https://huggingface.co/transformers/v3.2.0/model_doc/bert.html\n",
    "        # Pooler output is last layer of hidden state for [CLS] token, whose\n",
    "        # output is fed through a linear layer and a tanh function\n",
    "        #\n",
    "        # Shape of (batch_size, hidden_size) \n",
    "        output: torch.FloatTensor = self.linear(bert_output.pooler_output).squeeze()\n",
    "        loss = self.loss_fn(output, expected_score)\n",
    "        return {\n",
    "            \"norm_score\": output,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "    \n",
    "\n",
    "def bucketize_norm_scores(norm_scores: torch.FloatTensor, max_score: int) -> torch.IntTensor:\n",
    "    \"\"\"\n",
    "    Convert norm_scores into specific numbers\n",
    "    \"\"\"\n",
    "    return norm_scores // max_score\n",
    "\n",
    "\n",
    "class TrainingConfig:\n",
    "    def __init__(self, tokenizer: BertTokenizerFast, model: BertForMaskedLM, stop_delta_loss: float = 0.01, epoch_window: int = 5, name: str = \"model\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        # We stop when the average delta loss <= stop_delta_loss\n",
    "        # This average is taken over a window of size epoch_window \n",
    "        self.stop_delta_loss = stop_delta_loss\n",
    "        # Number of epochs from now to the past used to calculate the average delta loss\n",
    "        self.epoch_window = epoch_window\n",
    "        # The final classifier model, after finetuning\n",
    "        self.finetuned_classifier_model = cast(BERTSentimentClassifier, None)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def build_training_config(pretrained_model: bool = False, pretrained_model_name: str = \"bert-base-uncased\", stop_delta_loss: float = 0.01, epoch_window: int = 5, name: str = \"model\") -> TrainingConfig:\n",
    "    # NOTE: We are not re-training a tokenizer, since it's out of the scope of this experiment\n",
    "    tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
    "    if pretrained_model:\n",
    "        # Create tokenizer + already trained model\n",
    "        model: BertForMaskedLM = BertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "    else:\n",
    "        config = BertConfig(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            max_position_embeddings=512,\n",
    "            hidden_size=256,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=4,\n",
    "            type_vocab_size=2\n",
    "        )\n",
    "        model: BertForMaskedLM = BertForMaskedLM(config)\n",
    "    # Move the model to the device we speicified\n",
    "    #   Ideally use CUDA (GPU) if available\n",
    "    model.to(device)\n",
    "    return TrainingConfig(tokenizer=tokenizer, model=model, stop_delta_loss=stop_delta_loss, epoch_window=epoch_window, name=name)\n",
    "\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import TypedDict, cast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def move_dict_to(res: dict, device: torch.device):\n",
    "    for key in res:\n",
    "        if isinstance(res[key], torch.Tensor):\n",
    "            res[key] = cast(torch.Tensor, res[key]).to(device=device)\n",
    "\n",
    "\n",
    "class TokenizedInputs(TypedDict):\n",
    "    input_ids: torch.IntTensor\n",
    "    token_type_ids: torch.IntTensor\n",
    "    attention_mask: torch.IntTensor\n",
    "    labels: torch.IntTensor\n",
    "\n",
    "\n",
    "class MaskedTextDatasetItem(TokenizedInputs):\n",
    "    original_text: str\n",
    "\n",
    "\n",
    "class MaskedTextDataset(torch.utils.data.Dataset[MaskedTextDatasetItem]):\n",
    "    \"\"\"\n",
    "    Dataset of masked text\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: TokenizedInputs, original_text: list[str] = None, batch_device: torch.device = None):\n",
    "        self.encodings = encodings\n",
    "        self.original_text = original_text\n",
    "        self.batch_device = batch_device\n",
    "\n",
    "    def __getitem__(self, index: int) -> MaskedTextDatasetItem:\n",
    "        # Return the dictionary just like encodings, except it only\n",
    "        # contains the entries for a specific row (sentence)\n",
    "        res = {key: val[index] for key, val in self.encodings.items() }\n",
    "        if self.original_text:\n",
    "            res[\"original_text\"] = self.original_text[index]\n",
    "        # If output_device is set, we move individual batches to the device\n",
    "        if self.batch_device:\n",
    "            move_dict_to(res, self.batch_device)\n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        # NOTE: Moving entire dataset to GPU memory is a bad idea -- we don't have a lot of GPU memory\n",
    "        move_dict_to(self.encodings, device)\n",
    "\n",
    "\n",
    "class ReviewsDatasetItem(TokenizedInputs):\n",
    "    original_text: str\n",
    "    norm_score: float\n",
    "    score: int\n",
    "    max_score: int\n",
    "\n",
    "\n",
    "class ReviewsDataset(torch.utils.data.Dataset[ReviewsDatasetItem]):\n",
    "    \"\"\"\n",
    "    Dataset of reviews and their normaliezd scores (decimal number from 0 to 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: TokenizedInputs, scores: list[int], max_score: int, original_text: list[str] = None, batch_device: torch.device = None):\n",
    "        self.encodings = encodings\n",
    "        self.scores = scores\n",
    "        self.max_score = max_score\n",
    "        self.original_text = original_text\n",
    "        self.batch_device = batch_device\n",
    "    \n",
    "    def __getitem__(self, index: int) -> ReviewsDatasetItem:\n",
    "        # Return the dictionary just like encodings, except it only\n",
    "        # contains the entries for a specific row (sentence)\n",
    "        res = {key: val[index] for key, val in self.encodings.items() }\n",
    "        if self.original_text:\n",
    "            res[\"original_text\"] = self.original_text[index]\n",
    "        res[\"norm_score\"] = float(self.scores[index]) / self.max_score\n",
    "        res[\"score\"] = self.scores[index]\n",
    "        res[\"max_score\"] = self.max_score\n",
    "        # If output_device is set, we move individual batches to the device\n",
    "        if self.batch_device:\n",
    "            move_dict_to(res, self.batch_device)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        # NOTE: Moving entire dataset to GPU memory is a bad idea -- we don't have a lot of GPU memory\n",
    "        move_dict_to(self.encodings, device)\n",
    "\n",
    "\n",
    "def get_masked_dataset(tokenizer: BertTokenizerFast, review_texts: list[str], percent_masked: float = 0.15, dataset_file: str = \"\", batch_device: torch.device = None) -> MaskedTextDataset:\n",
    "    print(\"Loading masked dataset:\")\n",
    "    if dataset_file:\n",
    "        print(f\"    Loading existing dataset file @ {dataset_file}...\")\n",
    "        # Check if dataset file exists — if so, then load from file\n",
    "        try:\n",
    "            # Disable weights_only since we are loading aribitrary python classes\n",
    "            masked_text_dataset: MaskedTextDataset = torch.load(f\"{DATA_DIR}/{dataset_file}\", weights_only=False)\n",
    "            return masked_text_dataset\n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading dataset file: {e}\")\n",
    "    print(f\"    Creating new dataset file @ {dataset_file}...\")\n",
    "    encodings: TokenizedInputs = tokenizer(review_texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # Create a new field labels that is a clone of input_ids\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].detach().clone()\n",
    "    # In BERT paper, each token has 15% chance of being masked\n",
    "    # First, create random vector that spans all of the input_ids (spans all the tokens)\n",
    "    rand = torch.rand(encodings[\"input_ids\"].shape)\n",
    "    # (rand < 0.15) -> Any token that has a corresponding random value of < 0.15, we mask\n",
    "    # We also don't want to mask special tokens (101, 102), and padding tokens (0)\n",
    "    # * operator is elementwise multiplication, which is same as AND for boolean tensors\n",
    "    mask_arr = (rand < percent_masked) * (encodings[\"input_ids\"] != 101) * (encodings[\"input_ids\"] != 0) * (encodings[\"input_ids\"] != 102)\n",
    "    # Stores all the indices that we want to mask\n",
    "    masked_cols = []\n",
    "    # Iterate over each row in the mask_arr (basically each sentence in our text data)\n",
    "    for i in range(mask_arr.shape[0]):\n",
    "        # .nonzero() -> finds the indicies where we have \"true\" values (since true = 1 and false = 0 in pytorch)\n",
    "        masked_cols.append(mask_arr[i].nonzero().flatten().tolist())\n",
    "    # Apply our mask_arr in each row (each sentence)\n",
    "    for i in range(mask_arr.shape[0]):\n",
    "        # Special Tensor syntax -> we can pass in a list of indicies for any of the axes\n",
    "        #   In this case, we pass in a list of indices in the column axis, to effectively\n",
    "        #   select the columns (tokens) we want to mask out\n",
    "        encodings[\"input_ids\"][i, masked_cols[i]] = 103\n",
    "    masked_text_dataset = MaskedTextDataset(encodings, review_texts, batch_device=batch_device)\n",
    "    if dataset_file:\n",
    "        torch.save(masked_text_dataset, f\"{DATA_DIR}/{dataset_file}\")\n",
    "    return masked_text_dataset\n",
    "\n",
    "\n",
    "def get_reviews_dataset(tokenizer: BertTokenizerFast, review_scores: list[float], max_score: int, review_texts: list[str], dataset_file: str = \"\", batch_device: torch.device = None) -> ReviewsDataset:\n",
    "    print(\"Loading reviews dataset:\")\n",
    "    if dataset_file:\n",
    "        print(f\"    Loading existing dataset file @ {dataset_file}...\")\n",
    "        # Check if dataset file exists — if so, then load from file\n",
    "        try:\n",
    "            # Disable weights_only since we are loading aribitrary python classes\n",
    "            reviews_dataset: ReviewsDataset = torch.load(f\"{DATA_DIR}/{dataset_file}\", weights_only=False)\n",
    "            return reviews_dataset\n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading dataset file: {e}\")\n",
    "    encodings: TokenizedInputs = tokenizer(review_texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    reviews_dataset = ReviewsDataset(encodings, review_scores, max_score, review_texts, batch_device=batch_device)\n",
    "    print(f\"    Creating new dataset file @ {dataset_file}...\")\n",
    "    if dataset_file:\n",
    "        torch.save(reviews_dataset, f\"{DATA_DIR}/{dataset_file}\")\n",
    "    return reviews_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from typing import cast, Iterator\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import deque\n",
    "from typing import TypeVar, Generic\n",
    "import torch.utils.data as tdata\n",
    "import sklearn.model_selection as skms\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TDataset = TypeVar('TDataset', bound=tdata.Dataset)\n",
    "class TrainDatasetSplit(Generic[TDataset]):\n",
    "    def __init__(self, train: tdata.Subset[TDataset], test: tdata.Subset[TDataset], valid: tdata.Subset[TDataset]):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.valid = valid\n",
    "    \n",
    "    def get_dataloaders(self, batch_size: int = None, shuffle: bool = None, drop_last: bool = False) -> tuple[tdata.DataLoader, tdata.DataLoader, tdata.DataLoader]:\n",
    "        return (tdata.DataLoader(self.train, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last), \\\n",
    "                tdata.DataLoader(self.test, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last), \\\n",
    "                tdata.DataLoader(self.valid, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last))\n",
    "\n",
    "\n",
    "def train_split_dataset(dataset: TDataset, train_percent: float = 0.7, test_percent: float = 0.2, validate_percent: float = 0.1, random_state: int = None) -> TrainDatasetSplit[TDataset]:\n",
    "    assert math.isclose(train_percent + test_percent + validate_percent, 1.0), \"Expected train_percent + test_percent + validate_percent = 1.0!\"\n",
    "    dataset_indices = list(range(len(dataset)))\n",
    "    test_train_indices, valid_indicies = skms.train_test_split(dataset_indices, test_size=validate_percent, random_state=random_state)\n",
    "    train_indicies, test_indicies = skms.train_test_split(test_train_indices, test_size=test_percent/(train_percent + test_percent), random_state=random_state)\n",
    "    valid = tdata.Subset(dataset, valid_indicies)\n",
    "    train = tdata.Subset(dataset, train_indicies)\n",
    "    test = tdata.Subset(dataset, test_indicies)\n",
    "    split = TrainDatasetSplit(train=train, test=test, valid=valid)\n",
    "    return split\n",
    "\n",
    "\n",
    "def get_newest_model_file(prefix: str) -> str:\n",
    "    idx = -1\n",
    "    while os.path.exists(f\"{prefix}{idx + 1}.pt\"):\n",
    "        idx += 1\n",
    "    if idx >= 0:\n",
    "        return f\"{prefix}{idx}.pt\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "class TrainLoopIterData:\n",
    "    is_valid: bool\n",
    "\n",
    "    output_loss: torch.FloatTensor\n",
    "    output_num_correct: int\n",
    "\n",
    "    def reset(self, is_valid: bool = False):\n",
    "        self.output_loss = None\n",
    "        self.output_num_correct = None\n",
    "        self.is_valid = is_valid\n",
    "    \n",
    "    def postfix(self) -> dict:\n",
    "        return {\n",
    "            \"loss\": self.output_loss.item(),\n",
    "            \"num_correct\": self.output_num_correct\n",
    "        }\n",
    "\n",
    "\n",
    "class TrainLoopStats:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_loss = cast(float, None)\n",
    "        self.total_correct = cast(int, None)\n",
    "        self.average_loss = cast(float, 0)\n",
    "        self.accuracy = cast(float, 0)\n",
    "    \n",
    "    def add_iter_data(self, output: TrainLoopIterData):\n",
    "        if output.output_loss:\n",
    "            if self.total_loss == None:\n",
    "                self.total_loss = 0\n",
    "            self.total_loss += output.output_loss.item()\n",
    "        if output.output_num_correct:\n",
    "            if self.total_correct == None:\n",
    "                self.total_correct = 0\n",
    "            self.total_correct += output.output_num_correct\n",
    "\n",
    "    def calculate(self, batch_count: int, items_count: int):\n",
    "        self.average_loss = self.total_loss / batch_count if self.total_loss else None\n",
    "        self.accuracy = self.total_correct / items_count if self.total_correct else None\n",
    "\n",
    "\n",
    "TDatasetItem = TypeVar('TItem')\n",
    "TModel = TypeVar('TModel', bound=torch.nn.Module)\n",
    "def train_loop(name: str, desc: str, model_prefix: str, train_dataloader: tdata.DataLoader[TDatasetItem], valid_dataloader: tdata.DataLoader[TDatasetItem], model: TModel, optim: torch.optim.Optimizer, epoch_window: int, stop_delta_loss: float, train_fn: Callable[[TModel, TDatasetItem, TrainLoopIterData], None]):\n",
    "    log_data: list[dict] = []\n",
    "    init_epoch = 0\n",
    "    prev_average_valid_loss: float = None\n",
    "    \n",
    "    model_dir = f\"{MODEL_DIR}/{name}/\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    def calculate_delta_loss_moving_avg(log_data: list[dict]) -> float:\n",
    "        if len(log_data) <= 1:\n",
    "            return None\n",
    "        if len(log_data) <= epoch_window:\n",
    "            # Exclude the first entry if our window isn't large enough, since\n",
    "            # the first entry does not have a valid_loss_delta, as deltas\n",
    "            # are only calculated between entries.\n",
    "            log_data = log_data[1:]\n",
    "        return np.average([x[\"valid_loss_delta\"] for x in log_data[-epoch_window:]])\n",
    "\n",
    "    existing_model_file = get_newest_model_file(model_dir + model_prefix)\n",
    "    if existing_model_file:\n",
    "        # If model file exist, then try to continue off of where it left off\n",
    "        data = torch.load(existing_model_file)\n",
    "        model.load_state_dict(data[\"model\"])\n",
    "        optim.load_state_dict(data[\"optim\"])\n",
    "        init_epoch = data[\"log_data\"][-1][\"epoch\"] + 1\n",
    "        log_data = data[\"log_data\"]\n",
    "        prev_average_valid_loss = log_data[-1][\"valid_loss\"]\n",
    "        print(\"\")\n",
    "        delta_loss_moving_avg = calculate_delta_loss_moving_avg(log_data)\n",
    "        print(f\"Resuming existing model at epoch: {init_epoch}, valid loss: {log_data[-1][\"valid_loss\"]}, delta_loss: {delta_loss_moving_avg} <= {-stop_delta_loss}\")\n",
    "        if delta_loss_moving_avg != None and -delta_loss_moving_avg <= stop_delta_loss:\n",
    "            # We hit the cutoff, return early\n",
    "            return\n",
    "        # Otherwise, continue training\n",
    "\n",
    "    print(f\"Training start for '{name}' {desc}...\")\n",
    "    loop_iter_data = TrainLoopIterData()\n",
    "    epoch_loop = itertools.count(start=init_epoch, step=1)\n",
    "    for epoch in epoch_loop:\n",
    "        # Training loop\n",
    "        loop = tqdm(cast(Iterator[MaskedTextDatasetItem], train_dataloader), leave=True)\n",
    "        train_stats = TrainLoopStats()\n",
    "        loss_delta = None\n",
    "\n",
    "        model.train()\n",
    "        for data in loop:\n",
    "            loop_iter_data.reset(is_valid=False)\n",
    "\n",
    "            # Reset gradient\n",
    "            optim.zero_grad()\n",
    "\n",
    "            train_fn(model, data, loop_iter_data)\n",
    "            loss = loop_iter_data.output_loss\n",
    "            \n",
    "            # Apply backward propagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Set info in tqdm progress bar\n",
    "            loop.set_description(f\"Epoch: {epoch}\")\n",
    "            loop.set_postfix(loop_iter_data.postfix())\n",
    "            train_stats.add_iter_data(loop_iter_data)\n",
    "        train_stats.calculate(batch_count=len(train_dataloader), items_count=len(train_dataloader.dataset))\n",
    "\n",
    "        print(f\"Average training loss: {train_stats.average_loss}\")\n",
    "\n",
    "        # Validation loop\n",
    "        loop = tqdm(cast(Iterator[MaskedTextDatasetItem], valid_dataloader), leave=True)\n",
    "        valid_stats = TrainLoopStats()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in loop:\n",
    "                loop_iter_data.reset(is_valid=True)\n",
    "\n",
    "                train_fn(model, data, loop_iter_data)\n",
    "                loss = loop_iter_data.output_loss\n",
    "\n",
    "                # Set info in tqdm progress bar\n",
    "                loop.set_postfix(loop_iter_data.postfix())\n",
    "                valid_stats.add_iter_data(loop_iter_data)\n",
    "        valid_stats.calculate(batch_count=len(valid_dataloader), items_count=len(valid_dataloader.dataset))\n",
    "\n",
    "        print(f\"Average validation loss: {valid_stats.average_loss}\")\n",
    "\n",
    "        # Update moving average of validation loss\n",
    "        if prev_average_valid_loss != None:\n",
    "            loss_delta = valid_stats.average_loss - prev_average_valid_loss\n",
    "        else:\n",
    "            loss_delta = None\n",
    "        prev_average_valid_loss = valid_stats.average_loss\n",
    "\n",
    "        # Make plot\n",
    "        log_data_entry = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_stats.average_loss,\n",
    "            \"valid_loss\": valid_stats.average_loss,\n",
    "        }\n",
    "        if valid_stats.accuracy:\n",
    "            log_data_entry[\"valid_accuracy\"] = valid_stats.accuracy\n",
    "        if loss_delta != None:\n",
    "            log_data_entry[\"valid_loss_delta\"] = loss_delta\n",
    "        log_data.append(log_data_entry)\n",
    "        delta_loss_moving_avg = calculate_delta_loss_moving_avg(log_data)\n",
    "\n",
    "        epochs = [x[\"epoch\"] for x in log_data]\n",
    "        for key in log_data[0]:\n",
    "            if key.startswith(\"_\") or key == \"epoch\":\n",
    "                continue\n",
    "            plt.plot(epochs, [item[key] for item in log_data], label=key)\n",
    "        clear_output(wait=True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optim\": optim.state_dict(),\n",
    "                \"log_data\": log_data,\n",
    "            }, model_dir + model_prefix + f\"{epoch}.pt\")\n",
    "\n",
    "        # Break if our moving validation delta loss average is smaller than our stop_delta_loss\n",
    "        print(f\"Average delta loss ({epoch_window} epoch window): {delta_loss_moving_avg} <= {-stop_delta_loss}\")\n",
    "        if delta_loss_moving_avg != None and -delta_loss_moving_avg <= stop_delta_loss:\n",
    "            break\n",
    "\n",
    "\n",
    "def pretraining_loop(train_dataloader: tdata.DataLoader[MaskedTextDatasetItem], valid_dataloader: tdata.DataLoader[MaskedTextDatasetItem], config: TrainingConfig):\n",
    "    def train_fn(model: BertForMaskedLM, data: MaskedTextDatasetItem, iter_data: TrainLoopIterData) -> torch.FloatTensor:\n",
    "        outputs: MaskedLMOutput = model(input_ids=data[\"input_ids\"].to(device), attention_mask=data[\"attention_mask\"].to(device), labels=data[\"labels\"].to(device))\n",
    "        iter_data.output_loss = outputs.loss\n",
    "    \n",
    "    model = config.model\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    train_loop(name=config.name, \n",
    "        desc=\"pretraining\",\n",
    "        model_prefix=\"model_pretrain_\",\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        epoch_window=config.epoch_window,\n",
    "        stop_delta_loss=config.stop_delta_loss,\n",
    "        train_fn=train_fn)\n",
    "\n",
    "\n",
    "def finetuning_loop(train_dataloader: tdata.DataLoader[ReviewsDatasetItem], valid_dataloader: tdata.DataLoader[ReviewsDatasetItem], config: TrainingConfig):\n",
    "    def train_fn(model: BERTSentimentClassifier, data: ReviewsDatasetItem, iter_data: TrainLoopIterData) -> torch.FloatTensor:\n",
    "        output = model(input_ids=data[\"input_ids\"].to(device), attention_mask=data[\"attention_mask\"].to(device), expected_score=data[\"norm_score\"].to(dtype=torch.float32, device=device))\n",
    "        iter_data.output_loss = output[\"loss\"]\n",
    "        if iter_data.is_valid:\n",
    "            # Track accuracy for validation set\n",
    "            pred_score = bucketize_norm_scores(output[\"norm_score\"].to(device), data[\"max_score\"].to(device))\n",
    "            correct_preds = torch.count_nonzero(pred_score == data[\"score\"].to(device))\n",
    "            iter_data.output_num_correct = correct_preds\n",
    "\n",
    "    # Load data into a raw BERT model\n",
    "    # We set strict=False, since the pooler bias and weights are not set\n",
    "    bert_copy = BertModel.from_pretrained(\"bert-base-uncased\") # BertModel(config.model.bert.config)\n",
    "    \n",
    "    # Freeze all parameters (except for pooler)\n",
    "    for name, param in bert_copy.named_parameters():\n",
    "        param.requires_grad = \"pooler\" in name\n",
    "\n",
    "    # bert_copy.load_state_dict(config.model.bert.state_dict(), strict=False)\n",
    "    model = BERTSentimentClassifier(bert_copy)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    train_loop(name=config.name, \n",
    "        desc=\"finetuning\",\n",
    "        model_prefix=\"model_finetune_\",\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        epoch_window=config.epoch_window,\n",
    "        stop_delta_loss=config.stop_delta_loss,\n",
    "        train_fn=train_fn)\n",
    "    \n",
    "    config.model.finetuned_classifier_model = model\n",
    "\n",
    "\n",
    "def dataset_train_loop(dataset_name: str = \"myanimelist\") -> tuple[Callable[[], None], Callable[[float], None], Callable[[float], None]]:\n",
    "    TEST_PERCENT = 0.7\n",
    "    TRAIN_PERCENT = 0.2\n",
    "    VALIDATE_PERCENT = 0.1\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    data_df = pd.read_csv(f\"{DATA_DIR}/{dataset_name}_reviews.csv\")\n",
    "\n",
    "    review_texts = data_df[\"review\"].astype(str).to_list()\n",
    "    review_scores = data_df[\"score\"].astype(int).to_list()\n",
    "    review_max_score: int = data_df.loc[0, \"max_score\"]\n",
    "    \n",
    "    print(f\"Starting training for {dataset_name}...\")\n",
    "    print(f\"    review_texts: {review_texts[:3]}\")\n",
    "    print(f\"    review_scores: {review_scores[:3]}\")\n",
    "    print(f\"    review_max_score: {review_max_score}\")\n",
    "    \n",
    "    config: TrainingConfig = None\n",
    "    \n",
    "    def reset():\n",
    "        nonlocal config\n",
    "        config = build_training_config(name=dataset_name, stop_delta_loss=0.05, epoch_window=5)\n",
    "    \n",
    "    reset()\n",
    "    print(f\"Loaded training config:\")\n",
    "    display(config)\n",
    "    \n",
    "    masked_dataset = get_masked_dataset(config.tokenizer, review_texts, dataset_file=f\"{dataset_name}_masked_data.dt\")\n",
    "    def pretraining(stop_delta_loss: float = 0.05):\n",
    "        nonlocal config\n",
    "        config.stop_delta_loss = stop_delta_loss\n",
    "        reviews_split = train_split_dataset(masked_dataset, train_percent=TRAIN_PERCENT, test_percent=TEST_PERCENT, validate_percent=VALIDATE_PERCENT)\n",
    "        train_reviews_dataloader, test_reviews_dataloader, valid_reviews_dataloader = reviews_split.get_dataloaders(batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Masked pretraining:\")\n",
    "        print(f\"    train_dataloader: ({len(train_reviews_dataloader)} batches)\\n    test_dataloader: ({len(test_reviews_dataloader)} batches)\\n    valid_dataloader: ({len(valid_reviews_dataloader)} batches)\")\n",
    "        display(next(iter(train_reviews_dataloader)))\n",
    "        pretraining_loop(train_reviews_dataloader, valid_reviews_dataloader, config)    \n",
    "\n",
    "    reviews_dataset = get_reviews_dataset(config.tokenizer, review_scores, review_max_score, review_texts, dataset_file=f\"{dataset_name}_reviews_data.dt\")\n",
    "    def finetuning(stop_delta_loss: float = 0.05):\n",
    "        nonlocal config\n",
    "        config.stop_delta_loss = stop_delta_loss\n",
    "        reviews_split = train_split_dataset(reviews_dataset, train_percent=TRAIN_PERCENT, test_percent=TEST_PERCENT, validate_percent=VALIDATE_PERCENT)\n",
    "        train_reviews_dataloader, test_reviews_dataloader, valid_reviews_dataloader = reviews_split.get_dataloaders(batch_size=BATCH_SIZE, shuffle=True)\n",
    "        print(f\"Reviews training:\")\n",
    "        print(f\"    train_dataloader: ({len(train_reviews_dataloader)} batches)\\n    test_dataloader: ({len(test_reviews_dataloader)} batches)\\n    valid_dataloader: ({len(valid_reviews_dataloader)} batches)\")\n",
    "        display(next(iter(train_reviews_dataloader)))\n",
    "        finetuning_loop(train_reviews_dataloader, valid_reviews_dataloader, config)\n",
    "\n",
    "    return [reset, pretraining, finetuning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyAnimeList\n",
    "\n",
    "Training BERT on MyAnimeList only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for myanimelist...\n",
      "    review_texts: ['Oh dear Shingeki no Kyojin, where do I even begin. If you\\'ve talked with your friends about anime, then the couple anime that everyone talks about are Naruto, Bleach, One Piece, Dragon Ball, and... Shingeki no Kyojin. What\\'s the difference between Shingeki and the rest? Shingeki only has 25 episodes so far yet it\\'s on par in popularity with the other super long, Americanized anime. Why is it popular? Well that\\'s simply because it\\'s stunningly amazing. Those people that call Shingeki no Kyojin \"overrated\" may not have the same taste as me, and that\\'s perfectly fine, but in my honest opinion, Shingeki no Kyojin isone of if not the greatest anime to be made. It\\'s not popular for no reason.\\n\\nThe story is one of the most captivating stories I\\'ve ever seen. 100 years prior to the start of the anime, humanity has been on the bridge of extinction due to the monstrous humanoid Titans that devour humans. Now, present day in the anime, the remaining small population of mankind lives confined within 3 \"heavenly\" walls that are so tall and sturdy that even the titans can\\'t break in. The most outward wall was named, Wall Maria, the middle wall was named Wall Rose, and the most outward wall named Wall Sina. Unfortunately for mankind, a colossal titan, one that is even bigger than the 50 meter heavenly walls, breaks Wall Maria, allowing the other titans to rampage the city, thus leading to another massacre of mankind. During this massacre, our main characters, Eren Yeager and Mikasa Ackerman watch in horror as a horrifying titan rips their mother\\'s head off, then gobbles her up whole. Vowing that he\\'d one day avenge mankind and exterminate all the titans, Eren Yeager trains to become a survey corp, brave heroic soldiers who go outside the walls, into the plains in order to fight the titans. But we soon find out, that Eren is much more special than he seems, not only is he a brave warrior, but he\\'s also something else that could be the key to humanity\\'s survival, but could also be humanity\\'s destruction. \\n\\nMy 3-word thoughts on the anime: Epic, Dynamic, Masterpiece. The suspense build-up was absolutely amazing, yet there was still room for improvement; that shows just how epic this anime can get. The anime not only includes epic fights, but lots of dialogue, and for those of you that hate dialogue, I feel sorry for you people whom only watch for action. The anime includes lots of other things as well: there\\'s lots of half-hearted, hilarious scenes, as well as sad, tragic scenes. The anime certainly has a good amount of gore, and will break your heart frequently (if you get attached to the characters). Many characters end up getting gobbled up mercilessly while trying to protect humanity in ways that are quite *shivers*. Have I teared up in the anime? As a matter of fact, I have. \\n\\nCharacters was another area (alongside every area) that was Shingeki\\'s strongpoint. There\\'s a diverse variety of characters that fight for humanity for all different reasons. There\\'s trust, friendship, along with betrayals, and pains. There\\'s comedy relief among many characters, especially Sasha \"Potato Girl\" Blouse. There are characters that people can definitely relate with, such as Armin Arlert, who wants to do the right thing and protect his friends, but can\\'t seem to do much because of fear. Fear is something that haunts us all, and prevents us from doing things in life. Another character that people can somewhat relate with is Annie Leonhardt, who fights alongside humanity, yet fights opposed to humanity. What does that mean? Watch the anime. Why is she doing this? Because of her past scars, \"scars have the strange power to remind us that our past is real\" (watch to find out more). There were too many characters to development fully, but certainly the main characters were developed to their max. Levi is certainly a fan-favorite character, because of the fact that he\\'s cool, overpowered, badass, kind-in-the-heart, smart, straight-to-the-point, and most of all, hilarious with all his neat-freakiness. The main character, Eren Yeager is strong-hearted, and \"special\", but he\\'s still not strong enough to defeat the titans. How he develops is one of the most interesting things, in this interesting-things-packed anime. \\n\\nThe art and music can be described in one word: WOW! The art drawings were absolutely stunning! From the characters/titans to the setting of the story to all the equipment used in battle. The characters all had a unique aspect to them. The settings of the anime were beautiful. The cities, and walls looked realistic, the plains that characters dreamed of seeing made me want to run outside to see for myself how beautiful nature really is, and how humans under appreciate the naturality of nature. The 3-D gear was something that really caught my eye, and will certainly catch other people\\'s eyes as well. A new form of action that\\'s never been seen before. The characters would literally fly from rooftop to rooftop slicing their swords at the titans at high-speeds that keep the viewers eyes locked onto the screen at all times. The soundtrack in the anime was epic as shit. The openings speak for themselves, they do the anime justice. The openings were epic and certainly set the mood of what was to come from the anime. \"They\\'re the prey, and we are the hunters!\" The rest of the soundtrack in the anime was okayasgduyasgda AMAZING. How can one describe how amazing those German OSTs were. They fit in perfectly with the epicness of the anime, and certianly added tons of suspense to the 3-D maneuver gear action. \\n\\nOf course, I may be over-thinking things, but the anime certainly included some themes while creating this masterpiece of an anime. The aspect of being confined in an area, doing the same daily routine every day. People seek to be free, and to seek adventure. Watch for this. Another theme that the anime incorporated was that of the cycle of life. Humans, we steal animals away from their families, we kill them, we eat them. What\\'s so different from us, and the titans? The feelings of not being at the top of the food chain... Anyways, if you haven\\'t already watched this anime and you\\'re reading this review, then you\\'ve clearly been living under a rock all this time, and I definitely encourage you to watch this show even if it isn\\'t your style of anime.', 'I started to follow the manga after watching the anime and quite frankly I don\\'t see where and how the plot of Shingeki no Kyojin can suck, which in my humble opinion is awsome. You can understand the anime very well even without having read the manga. However, SNK isn\\'t understood by many because it\\'s a psychological manga .. SNK isn\\'t just a succession of empty and meaningless clashes and, of uncontrolled violence, stereotyped heroes who fight the bad (like Bleach). Here are shown the darkest sides of the human psyche with great mastery: the desire to be the best but can\\'t do it, thedifference between men, their different modes of action (just think about the speech when Eren was discovered in which they show the reactions of the inhabitants of the inner walls Rose) and their motivations, some of the questionable actions that seems to be made in the right and instead the Justice of others that superficially seems foolish and petty (such as the choice of sending refugees to rebuild the wall Mary), freedom and the fact that they don\\'t understand that what others inculcate to us sometimes are nothing more than mental cages. We stop at the appearances only because it suits us ( the walls is a symbol of that). Omitting the main and masterfully developed theme of \"Homo omini lupus\".\\nIn this anime there are no heroes! There are not brave men, without fear, that kill all the wicked. Here there are people who are fighting against their own fear and that sometimes they make it and sometimes not. The enemies aren\\'t the giants !Although the anime does not say from where they come from and I won\\'t do spoilers, it\\'s repeated several times that the enemies are the human beings with their fear (who have surrendered to the giants without doing anything, ignoring and accepting to live like cattle) and the walls! But no one notices that because it would be too challenging to make two questions and wondering: \"Why the walls ? Shouldn\\'t they be their salvation?\" and get an answer.\\nThe fact that many people aren\\'t well characterized, and that physically are not very different from each other isn\\'t a coincidence .. Isayama is putting on the same level all human beings, he is saying that we are all like that guy who shoots himself alone after he surrendered to fear, that we are all as Vernam (who during the attack fled from Trost ), that there is some of us who is better than others, but that we all live under the same fears, and even if we react differently, we act according to substantially the same nature. It still tells us that we might be the protagonists of our life, choosing our destiny, but only if we are ready to go beyond the stereotypes and mindsets that others impose to us, only if only if we work with all our strength. Because living by what others tell us, from what they show us, isn\\'t to live, but survive!\\nAnd in addition to all the various points of reflection that SNK has to offer (and there are many, not just those listed ), there is a plot full of twists and turns, the suspense. In short, it is a masterpiece. \\nThen, if you are used to an anime where the bad guys magically becomes good after a lesson or in which the characters acquire super powers from nothing or from old men who appearing in their minds... Well, ok ...... that ... that\\'s original and profound ........\\n(Hope my english was not so bad) :)', 'In the 80\\'s, Mobile Suit Gundam catapulted anime in Japan. In the 90s, Dragon Ball Z broke the mainstream walls while Neon Genesis Evangelion opened the doors to the now indispensable late-night anime slots in Japan. In the 2000s, Fullmetal Alchemist marked the peak of manga-to-anime adaptations while Code Geass & Death Note headed the class of those who introduced the new anti-hero genre. And now, in 2013, Attack on Titan has shaken the industry once again. Unless you\\'ve been living under a rock for the past 5 months, you should at least know what the story is about, and for the sake of staying spoiler free, I\\'m notgoing into details on the plot.\\n\\nBefore I wrote this review, I wrote a different piece moments after watching the first episode stating that Attack on Titan had all the potential to become a landmark anime, knowing very little of how true that statement would turn out to be.\\n\\nI have watched anime ever since I can remember. I have seen the very best and the worst that this industry has to offer. And I, like most of you, am very familiar with the relationship of manga-anime adaptation, which is the case of Attack on Titan. It is because of such experience and knowledge that I am able to say with so much confidence that Attack on Titan is the best anime series ever produced from any standpoint, at least, in the last decade.\\n\\nAttack on Titan is such a well-done product that it has all the key strong points we all dream every series could have. By this I mean the story, characters, animation, OST, opening/ending sequences, art, directing, narrative, character development, and, most importantly, the \"it\" factor. These are all present with \"A grade\" production values. \\n\\nFirst of all, I want to start with the OST. Simply put, Attack on Titan\\'s soundtrack rivals that of Rurouni Kenshin and Evangelion. Period. Don\\'t misunderstand, they are not similar by any means, they are simply equally as masterful. Check on the internet the impressions left by fans about the first soundtrack. And if you still don\\'t believe me, tweet Hideo Kojima (creator of Metal Gear series) and ask his take on it.\\n\\nThe OST is extremely important in this show because it allows you to feel the many epic moments that dwell in this 25 episode marvel. This is where the directing and narrative play a big role. For those of you who have seen Death Note (same director), you will feel an extra feeling of similarity with Attack on Titan, as the show relies heavily on its intense emotional scenes which you most certainly will feel the first 2 minutes of episode #1. \\n\\nIt is in these scenes where the golden cast of characters shine. I cannot stress enough how unique the characters are. You could say they are the cream of the many accolades that Attack on Titan has. I\\'m still amazed by the quality of voice actors that just seem to just pop out in this show. The voices of so many main characters from hit series come together in Attack on Titan to help provide that epic feeling you start to get once episode #1 ends. Note I emphasize the voice acting because it\\'s part of the anime but, obviously, not of the manga. \\n\\nAnother key strong point is the work and effort put on the OP/ED sequences. If you\\'ve come this far in my review then you most likely understand when I say that Openings and Ending sequences are crucial. They provide that extra excitement to the show. It\\'s definitely something not all anime series take seriously. Fortunately, Wit Studio took them seriously, and then came \"Guren no Yumiya\". The second opening, \"Jiyuu no Tsubasa\" is outstanding and marvelous in its own way and the second ending, \"great escape\", fuels that adrenaline rush of excitement you will get after those killing cliffhangers. Having said that, the first OP, \"Guren no Yumiya\", is simply epic. If there was ever something to describe as epic, it would be this OP. I am certain that as of today, it is the greatest anime opening ever. I\\'m amazed by the amount of attention it got on the internet and the hundreds of parodies derived from it. It was simply a treat to our eyes. The song. The animation in the opening. The sync between the two. Epic. \\n\\nYou won\\'t be skipping that opening.\\n\\nIn addition, an animation production is never as close to perfection without the animation and art being top class. Now, Attack on Titan has astonishing art, to the point where you are amazed of how far animation has come. The scenes where there is sunlight will leave you speechless. The animation, unlike the other aspects, is where some disagree. The only thing I have to say is I was extremely impressed throughout the whole show until I saw a certain scene in episode #11 where the 3D maneuver gear was used, and I was stunned. I was simply stunned. Then, of course, later episodes also take it to a whole new level, but you\\'re probably gonna be used to that quality of greatness by then.\\n\\nThat\\'s how ridiculously good Attack on Titan is.\\n\\nNow, like the characters, the story is mostly work of the manga\\'s author. This includes character development, which I believe is the key to the success and extreme popularity of the characters. Also, the story maybe the most original aspect of this series. Nothing you have seen before is in this show. Nothing. Much like EVA, Attack on Titan gives you a world of \"dystopia\", on-edge, and uncertainty, so full of potential that just when you think you have it figured it out, it turns around and leaves you speechless. \\n\\nOh, that will happen to you for the first time in episode #5. Guaranteed. \\n\\nFinally, even IF all these characteristics of an anime series are top notch, it won\\'t amount to its potential if it doesn\\'t have that \"IT\" factor. Some series have the \"IT\" factor without having all of these production values. They tend have 3 or 4 at most. Those become popular because they give out that feeling of \"amazing\". Gintama, the greatest gag series ever, comes to mind. Its anime doesn\\'t have overwhelming OST, or eye popping animation; it does have A class characters and A class story (among others), but that \"IT\" factor takes it to another level. Now, imagine having practically everything an anime series has to offer in A class value while also having that \"IT\" factor. Extremely rare. Eva rare. FMA rare. Attack on Titan is part of that elite group. This is why its popularity rose to highs only few series reach.\\n\\nThis is the first time I have ever written a detailed review of an anime series. I did it because this one deserves it. It\\'s The anime series of my adult era. I can now relate to those anime fans who watched Evangelion almost 20 years ago. They knew they had something special, just like we do now with Attack on Titan. \\n\\nIn conclusion, Attack on Titan is, of course, like MS Gundam, DBZ, EVA, FMA, Geass, and Death Note, not perfect. Like any other piece of greatness it does have its negatives. Having said that, when you add up the good and the bad, you end up with a landmark anime series that is currently shaking the industry with its mesmerizing, sensational, emotional, intense, and inspiring first season. Just imagine the wait and the hype for season 2. \\n\\nWhat kind of effect will Attack on Titan have on the anime/manga industry we love? Only time will tell...\\n\\nIn the meantime, this is simply the best anime series in the last 15 years. Enjoy.\\n\\nExquisite beyond words.\\n101/100.']\n",
      "    review_scores: [10, 10, 10]\n",
      "    review_max_score: 10\n",
      "Loaded training config:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokenizer': BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), 'model': BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       "), 'name': 'myanimelist', 'stop_delta_loss': 0.05, 'epoch_window': 5, 'finetuned_classifier_model': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading masked dataset:\n",
      "    Loading existing dataset file @ myanimelist_masked_data.dt...\n",
      "Loading reviews dataset:\n",
      "    Loading existing dataset file @ myanimelist_reviews_data.dt...\n"
     ]
    }
   ],
   "source": [
    "reset, pretraining, finetuning = dataset_train_loop(\"myanimelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average delta loss (5 epoch window): -0.0010693504696803302 <= 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "reset()\n",
    "pretraining()\n",
    "finetuning(-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steam\n",
    "\n",
    "Training BERT on Steam only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for steam...\n",
      "    review_texts: [\"Amazing game. Easily 30-40 hours of game play. I hope the dev continues to add more depth in the future! I'd love to see,- More zones, cities, villages etc- Additional starting scenarios (in debt, being a dealer for someone else, certain supplies/drugs not available)- Setups (undercover cops/narcs)- Raids/counter-raids (Police/cartel)- Thief's stealing supplies (if door left open, or employees turning against you)- Mixing drugs with other drugs (weed w/ cocaine)- Turf wars w/ opponent drug-lord organisation charts that you can war with, slowly discover their hierarchy and order hits or do them yourself- High volume deals (shipping supplies off to other regions)\", 'For a just \"released in early access game\" the devs managed to make a game that works, have a fun gameplay loop and satisfying pacing in growth. I would like to see:- a search function in the product management app- a way to change the strain\\'s name after the fact, not only when you \\'discover\\' it. - some one or faction being an obstacle. My RV gets blown to kingdom come with a dire warning, and I just go on to become a king-pin without hesitation? - I\\'d like to see the Popo chase NPCs too. There is this very important hardcore curfew! for... the PC, and no one else. - More interactions. I liked getting the quests and tasks with a little bit of story, but once you hit a certain point around the time you\\'re ready to buy your first piece of real estate that isnt renting the motel or the space above the restaurant, it\\'s lacking. Devs, Good show, and good luck! At this point, I have already gotten my money\\'s worth already, but I hope to see more. Mushrooms are fascincating, arent they?', 'day time comes: pickpocket the whole city night time comes: go gambling with the boys (drugs sold 0) 10/10 game']\n",
      "    review_scores: [1, 1, 1]\n",
      "    review_max_score: 1\n",
      "Loaded training config:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokenizer': BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), 'model': BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       "), 'name': 'steam', 'stop_delta_loss': 0.05, 'epoch_window': 5, 'finetuned_classifier_model': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading masked dataset:\n",
      "    Loading existing dataset file @ steam_masked_data.dt...\n",
      "    Error loading dataset file: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Alan\\\\Desktop\\\\Open_Source\\\\BERT-TLSA-paper/data/steam_masked_data.dt'\n",
      "    Creating new dataset file @ steam_masked_data.dt...\n"
     ]
    }
   ],
   "source": [
    "dataset_train_loop(\"steam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacritic\n",
    "\n",
    "Training BERT on Metacritic only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Boring game that was soulless even in the beginning. Its contemporaries Roadblocks (why is this censored lol) and Blockland do everything better. Easily the most overrated game of all time.',\n",
       " 'the best, the only, the unique game in its genre.\\nThis game gave me my childhood, THIS IS THE BEST GAME',\n",
       " 'Not my cup of tea due to so many choices. Still pretty great. Memorable style of world. Diverse & expansive gameplay. Major impact on the industry.',\n",
       " 'Film He was Very average More I gave Credit By the References And also For the Nostalgia The jokes They are not Funny One Except Put Some well Specific More on al He was Good More or less',\n",
       " 'Minecraft é muito criativo, dá muita liberdade para os jogadores, possui um vasto lore próprio, muitos detalhes, investimento e desenvolvimento contínuo ao longo de anos após o lançamento, meus amigo sempre gostaram, sempre foi muito popular no youtube e na twitch...\\n\\nE mesmo assim eu nunca gostei tanto. Talvez pelo gênero de sobrevivência que não é muito a minha praia.',\n",
       " 'i just loved crushing loaf too much from the movie so of course im a absolute steve glazer. \"THESE GUYS ARE THE VILLAGERS, THEY LOOOOOOOOVE CRUSHING LOAF\"',\n",
       " 'Spent 30$ on it to play with my friends on a world, they had already been playing it so I was kind of behind ended up dying in the first 30 minutes to a glitch ruined my week',\n",
       " 'Лучшая песочница всех времён, лучший саундтрек, да и просто спасибо за детство.',\n",
       " \"I've finally watched the movie and I loved it. (Sorry for the haters)\\nOf course there were some problems in it, like the zombies that are moving like a human was in a costume. But despite of that, the movie is great. \\n\\nI won't say it is the movie of the year but it deserves to be watched...\",\n",
       " \"Minecraft 2025 is a colossal misfire that makes you wonder how anyone could fumble a property as endlessly creative as Minecraft this badly especially when Super Mario proved video game adaptations can be vibrant, fun, and worth watching. Instead of a blocky wonderland bursting with imagination, we get a soulless, corporate cash-grab that's as uninspired as a dirt hut built by a first-day player. And at the center of this mess is Jack Black, whose over-the-top shtick as Steve feels less like a character and more like a bloated, annoying caricature of himself phoning it in with the energy **** who ate too many pork chops and forgot what made his Bowser a good character.Where Super Mario delivered a tight, colorful romp with a villain you couldn't help but like to hate, Minecraft stumbles through a plot so thin it could've been scribbled on a 3x5 card with a fat sharpie. The live action gimmick is a disaster with corpulent Jack Black waddling around in a turquoise shirt against a green screen looks less like an adventure and more like a sad clown act at a kid's party no one showed up to. His constant mugging and forced musical numbers grate on the nerves, turning Steve into a flabby, loud distraction instead of the everyman hero Minecraft fans **** visuals? A garish mishmash of CGI and real world actors that clash worse than a creeper in a flower biome. The charm of Minecraft's pixelated freedom is buried under a generic 'save the world' story that feels like it was ripped from a dozen better films none of which bothered to star a Jack Black who seems to think yelling and flailing is a substitute for depth. Super Mario knew how to balance nostalgia with heart; this just shovels Easter eggs and tired gags into a pit and calls it a day. It's not just terrible it's a betrayal of everything that makes Minecraft special, leaving you longing for the Mushroom Kingdom instead of this blockheaded bore.TLDR - Jack Black ruined this movie and he is a bloated slob who phoned it in.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_loop(\"metacritic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes\n",
    "\n",
    "Training BERT on Rotten Tomatoes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Una obra de arte dirigida por Denis Villeneuve. Esta película es una de las mejores que he visto en este género. Sin necesidad de leer los libros, comprenderás a la perfección todo el contexto. El director busca deleitarnos con sus tomas cinematográficas y su espectacular sonido ambiental. Una obra de arte digna de un Oscar. El único problema que le veo, y por el que no le doy un 10, es el comienzo. Puede ser muy tedioso, pero se entiende perfectamente por qué.\\n\\nConclusión: Esta es una película que definitivamente tienes que ver en algún momento. El comienzo puede ser lento porque te da mucho contexto. No pierdas la oportunidad de verla.\\n\\nCalificación: 8.5/10',\n",
       " 'Took me a while to get into it, but as someone who has not read the Dune books, it truly feels like a book. I can see the influence and see how the book must have been like, I definitely plan to read the books at some point. Some of the decisions are a bit sub-par though. They should 100% have had a different cast. Yes, sure, they chose some good actors, but starring Zendaya in something like this is an absolute no. And I LOVE David Batista, but all I could think any time he appeared on screen, is Oh Look David Batista. You have to cast less known actors in big fantasy movies like this, because it just takes the watcher out of the movie, it takes you OUT of the beautiful world building when Zendaya with weird eyes appears on screen every so often. Completely jolts you out of the fantasy and back to \"Oh. Zendaya looks kinda weird in this movie.\" or \"haha look its David Batista\".',\n",
       " 'This might be my favorite movie !\\nDune (2021) is a sci-fi movie adapted from a roman directed by Dennis Villeneuve with Timothée chalamet as Paul Atréides  and Zendaya as Chani\\n\\nIn Dune(2021) Paul Atréides follows his very important and powerful family to the desertic planet of Arrakis gifted by the Imperium , where a very important ressource is collected and put on the market, but after an conspiracy and a betrayal Paul have to flee on the planet where he will met the native désert’s people, the Fremen\\n\\nThe actors were chosen perfectly and respected the physical descriptions given in the roman with the exception of some like Liet Kynes who was played by a woman. I also loved the makeup art on the Harkonnen family and the Eyes of Ibad\\n\\nThe film technique were amazing ! I think we cannot represent Arrakis better than they did , the lighting were very good on the desert and the pictures and ambiance were perfect \\nThe soundtrack by Hans Zimmer was flawless !\\nI really loved the costumes, from the Paul outfit on Caladan or Jessica outfit on the Arrakis arrival to the distille and Harkonnen Armor\\nAnd that’s why this movie won 6 oscars for 10 nominations\\n\\nDune explore theme like ecology power and colonialism.The film shows how the struggle for the rare ressource spice touch political, economic, and cultural conflicts, mirroring real-world tensions over oil and natural resources.  And my favorite theme is religion because its a big interest to me at this moment in my life, i love debatting about religion, learning about the story of religion so i absolutly loved the religion thème and everything its brings to the story \\n\\nFinally i really recommend to the movie watcher to try to read dune !!! Even though the adaptation is perfect in therm of cinematography i think the plot is very hard to fully understand and the movie miss a lot of scene and aspect of the story . So as a lover of Dune universe i fully recommend the book before watching the movie !',\n",
       " 'Extremely slow & boring, most action occurs in the dark and is hard to see. I was 2 hours into it before it grabbed my interest then was very anticlimactic.',\n",
       " 'Exceptional, wildly fantastic and never a dull moment. Truly spectacular in every possible way a movie can be. Cast all excels in character, cinematography out of this world. Worth the wait, for sure.',\n",
       " 'EPIC SCALE MOVIE\\nCGI WAS AMAZING, as well as the score, acting, plot, and cinematography.\\nthe movie overstayed its welcome a little bit, with some scenes dragging out the film to its colossal 2:30 runtime. this being said, the movie was great and i would definitely recommend to any drama or sci fi lovers.',\n",
       " 'Um espetáculo. Na minha opinião um dos melhores filmes dos últimos anos.',\n",
       " \"I've watched 👀 this movie at least ten times, that should say Everything!!!\",\n",
       " 'Not bad. I still prefer the classic one from 1984 though (which I\\'ve just rewatched). I can\\'t make comparisons with the books because I didn\\'t read them, so I don\\'t know how much the films differ from the original story. I can only make a comparison with the first film and I definitely liked more the characters of that one. I\\'ve found them having more personality. In this one I find the characters more \"flat\". It\\'s still not bad though, good special effects and interesting lore',\n",
       " 'A slogfest of a story, without any exciting moments to consider this movie memorable. The 1.5 stars are for the fact that the movie is undeniably beautiful with incredible sound. Feels like this movie was made to set up for the vastly superior second chapter.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_loop(\"rotten_tomatoes\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
