{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwdIxdbsvqd8"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Code\n",
    "\n",
    "Common functions for training and fine-tuning the BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT module_path:  c:\\Users\\Alan\\Desktop\\Open_Source\\BERT-TLSA-paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(\"INIT module_path: \", module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "DATA_DIR = module_path + \"/data\"\n",
    "MODEL_DIR = module_path + \"/model\"\n",
    "\n",
    "for data_dir in [DATA_DIR, MODEL_DIR]:\n",
    "    os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM, BertModel, BertConfig\n",
    "import torch\n",
    "import enum\n",
    "from typing import cast\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class BERTSentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert: BertModel):\n",
    "        self.bert = bert\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bert.config.hidden_size, 1), # Convert from the hidden state to a single output\n",
    "            torch.nn.Sigmoid() # Constrain output between 0 and 1\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, expected_rating: torch.FloatTensor, *args, **kwargs) -> dict:\n",
    "        bert_output = self.bert(*args, **kwargs)\n",
    "        # https://huggingface.co/transformers/v3.2.0/model_doc/bert.html\n",
    "        # Pooler output is last layer of hidden state for [CLS] token, whose\n",
    "        # output is fed through a linear layer and a tanh function\n",
    "        #\n",
    "        # Shape of (batch_size, hidden_size) \n",
    "        output: torch.FloatTensor = self.linear(bert_output.pooler_output)\n",
    "        loss = self.loss_fn(output, expected_rating)\n",
    "        return {\n",
    "            \"norm_rating\": output,\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "    \n",
    "\n",
    "def bucketize_norm_ratings(norm_ratings: torch.FloatTensor, max_score: int) -> torch.IntTensor:\n",
    "    \"\"\"\n",
    "    Convert norm_ratings into specific numbers\n",
    "    \"\"\"\n",
    "    bucket_size = 1 / max_score\n",
    "    return torch.div(norm_ratings, bucket_size, rounding_mode=\"floor\")\n",
    "\n",
    "\n",
    "class TrainingConfig:\n",
    "    class StopMode(enum.Enum):\n",
    "        EPOCH = 0,\n",
    "        DELTA_LOSS = 1,\n",
    "\n",
    "    def __init__(self, tokenizer: BertTokenizerFast, model: BertForMaskedLM, mode: StopMode = StopMode.EPOCH, epochs: int = 20, stop_delta_loss: float = 0.01, epoch_window: int = 5, name: str = \"model\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.reset_optim()\n",
    "        self.epochs = epochs\n",
    "        self.name = name\n",
    "        self.mode = mode\n",
    "        # We stop when the average delta loss <= stop_delta_loss\n",
    "        # This average is taken over a window of size epoch_window \n",
    "        self.stop_delta_loss = stop_delta_loss\n",
    "        # Number of epochs from now to the past used to calculate the average delta loss\n",
    "        self.epoch_window = epoch_window\n",
    "        # The final classifier model, after finetuning\n",
    "        self.finetuned_classifier_model = cast(BERTSentimentClassifier, None)\n",
    "    \n",
    "    def reset_optim(self):\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def build_training_config(pretrained_model: bool = False, pretrained_model_name: str = \"bert-base-uncased\", mode: TrainingConfig.StopMode = TrainingConfig.StopMode.EPOCH, epochs: int = 20, stop_delta_loss: float = 0.01, epoch_window: int = 5, name: str = \"model\") -> TrainingConfig:\n",
    "    # NOTE: We are not re-training a tokenizer, since it's out of the scope of this experiment\n",
    "    tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
    "    if pretrained_model:\n",
    "        # Create tokenizer + already trained model\n",
    "        model: BertForMaskedLM = BertForMaskedLM.from_pretrained(pretrained_model_name)\n",
    "    else:\n",
    "        config = BertConfig(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            max_position_embeddings=512,\n",
    "            hidden_size=256,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=4,\n",
    "            type_vocab_size=2,\n",
    "        )\n",
    "        model: BertForMaskedLM = BertForMaskedLM(config)\n",
    "    # Move the model to the device we speicified\n",
    "    #   Ideally use CUDA (GPU) if available\n",
    "    model.to(device)\n",
    "    return TrainingConfig(tokenizer=tokenizer, model=model, mode=mode, epochs=epochs, stop_delta_loss=stop_delta_loss, epoch_window=epoch_window, name=name)\n",
    "\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import TypedDict, cast\n",
    "\n",
    "\n",
    "class TokenizedInputs(TypedDict):\n",
    "    input_ids: torch.IntTensor\n",
    "    token_type_ids: torch.IntTensor\n",
    "    attention_mask: torch.IntTensor\n",
    "    labels: torch.IntTensor\n",
    "\n",
    "\n",
    "class MaskedTextDatasetItem(TokenizedInputs):\n",
    "    original_text: str\n",
    "\n",
    "\n",
    "class MaskedTextDataset(torch.utils.data.Dataset[MaskedTextDatasetItem]):\n",
    "    \"\"\"\n",
    "    Dataset of masked text\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: TokenizedInputs, original_text: list[str] = None):\n",
    "        self.encodings = encodings\n",
    "        self.original_text = original_text\n",
    "\n",
    "    def __getitem__(self, index: int) -> MaskedTextDatasetItem:\n",
    "        # Return the dictionary just like encodings, except it only\n",
    "        # contains the entries for a specific row (sentence)\n",
    "        res = {key: val[index] for key, val in self.encodings.items() }\n",
    "        if self.original_text:\n",
    "            res[\"original_text\"] = self.original_text[index]\n",
    "        return res\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        for key in self.encodings:\n",
    "            if isinstance(self.encodings[key], torch.Tensor):\n",
    "                self.encodings[key] = cast(torch.Tensor, self.encodings[key]).to(device=device)\n",
    "\n",
    "\n",
    "class ReviewsDatasetItem(TokenizedInputs):\n",
    "    original_text: str\n",
    "    norm_rating: float\n",
    "    rating: int\n",
    "    max_score: int\n",
    "\n",
    "\n",
    "class ReviewsDataset(torch.utils.data.Dataset[ReviewsDatasetItem]):\n",
    "    \"\"\"\n",
    "    Dataset of reviews and their normaliezd ratings (decimal number from 0 to 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings: TokenizedInputs, ratings: list[int], max_score: int, original_text: list[str] = None):\n",
    "        self.encodings = encodings\n",
    "        self.ratings = ratings\n",
    "        self.max_score = max_score\n",
    "        self.original_text = original_text\n",
    "    \n",
    "    def __getitem__(self, index: int) -> ReviewsDatasetItem:\n",
    "        # Return the dictionary just like encodings, except it only\n",
    "        # contains the entries for a specific row (sentence)\n",
    "        res = {key: val[index] for key, val in self.encodings.items() }\n",
    "        if self.original_text:\n",
    "            res[\"original_text\"] = self.original_text[index]\n",
    "        res[\"norm_rating\"] = float(self.ratings[index]) / self.max_score\n",
    "        res[\"rating\"] = self.ratings[index]\n",
    "        res[\"max_score\"] = self.max_score\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "    def to(self, device: torch.device):\n",
    "        for key in self.encodings:\n",
    "            if isinstance(self.encodings[key], torch.Tensor):\n",
    "                self.encodings[key] = cast(torch.Tensor, self.encodings[key]).to(device=device)\n",
    "\n",
    "\n",
    "def read_review_texts(data_file: str = \"myanimelist_reviews.csv\") -> list[str]:\n",
    "    data_df = pd.read_csv(f\"{DATA_DIR}/{data_file}\")\n",
    "    return data_df[\"review\"].astype(str).to_list()\n",
    "\n",
    "\n",
    "def get_masked_dataset(tokenizer: BertTokenizerFast, review_texts: list[str], percent_masked: float = 0.15, dataset_file: str = \"\") -> MaskedTextDataset:\n",
    "    print(\"get_masked_dataset:\")\n",
    "    if dataset_file:\n",
    "        print(f\"    Loading existing dataset file @ {dataset_file}...\")\n",
    "        # Check if dataset file exists ‚Äî if so, then load from file\n",
    "        try:\n",
    "            # Disable weights_only since we are loading aribitrary python classes\n",
    "            masked_text_dataset: MaskedTextDataset = torch.load(f\"{DATA_DIR}/{dataset_file}\", weights_only=False)\n",
    "            return masked_text_dataset\n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading dataset file: {e}\")\n",
    "    print(f\"    Creating new dataset file @ {dataset_file}...\")\n",
    "    encodings: TokenizedInputs = tokenizer(review_texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # Create a new field labels that is a clone of input_ids\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].detach().clone()\n",
    "    # In BERT paper, each token has 15% chance of being masked\n",
    "    # First, create random vector that spans all of the input_ids (spans all the tokens)\n",
    "    rand = torch.rand(encodings[\"input_ids\"].shape)\n",
    "    # (rand < 0.15) -> Any token that has a corresponding random value of < 0.15, we mask\n",
    "    # We also don't want to mask special tokens (101, 102), and padding tokens (0)\n",
    "    # * operator is elementwise multiplication, which is same as AND for boolean tensors\n",
    "    mask_arr = (rand < percent_masked) * (encodings[\"input_ids\"] != 101) * (encodings[\"input_ids\"] != 0) * (encodings[\"input_ids\"] != 102)\n",
    "    # Stores all the indices that we want to mask\n",
    "    masked_cols = []\n",
    "    # Iterate over each row in the mask_arr (basically each sentence in our text data)\n",
    "    for i in range(mask_arr.shape[0]):\n",
    "        # .nonzero() -> finds the indicies where we have \"true\" values (since true = 1 and false = 0 in pytorch)\n",
    "        masked_cols.append(mask_arr[i].nonzero().flatten().tolist())\n",
    "    # Apply our mask_arr in each row (each sentence)\n",
    "    for i in range(mask_arr.shape[0]):\n",
    "        # Special Tensor syntax -> we can pass in a list of indicies for any of the axes\n",
    "        #   In this case, we pass in a list of indices in the column axis, to effectively\n",
    "        #   select the columns (tokens) we want to mask out\n",
    "        encodings[\"input_ids\"][i, masked_cols[i]] = 103\n",
    "    masked_text_dataset = MaskedTextDataset(encodings, review_texts)\n",
    "    if dataset_file:\n",
    "        torch.save(masked_text_dataset, f\"{DATA_DIR}/{dataset_file}\")\n",
    "    return masked_text_dataset\n",
    "\n",
    "\n",
    "def get_reviews_dataset(tokenizer: BertTokenizerFast, norm_ratings: list[float], review_texts: list[str], dataset_file: str = \"\") -> ReviewsDataset:\n",
    "    print(\"get_reviews_dataset:\")\n",
    "    if dataset_file:\n",
    "        print(f\"    Loading existing dataset file @ {dataset_file}...\")\n",
    "        # Check if dataset file exists ‚Äî if so, then load from file\n",
    "        try:\n",
    "            # Disable weights_only since we are loading aribitrary python classes\n",
    "            reviews_dataset: ReviewsDataset = torch.load(f\"{DATA_DIR}/{dataset_file}\", weights_only=False)\n",
    "            return reviews_dataset\n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading dataset file: {e}\")\n",
    "    encodings: TokenizedInputs = tokenizer(review_texts, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    reviews_dataset = ReviewsDataset(encodings, norm_ratings, review_texts)\n",
    "    print(f\"    Creating new dataset file @ {dataset_file}...\")\n",
    "    if dataset_file:\n",
    "        torch.save(reviews_dataset, f\"{DATA_DIR}/{dataset_file}\")\n",
    "    return reviews_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from typing import cast, Iterator\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import deque\n",
    "from typing import TypeVar, Generic\n",
    "import torch.utils.data as tdata\n",
    "import sklearn.model_selection as skms\n",
    "import math\n",
    "import copy\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TDataset = TypeVar('T', bound=tdata.Dataset)\n",
    "class TrainDatasetSplit(Generic[TDataset]):\n",
    "    def __init__(self, train: tdata.Subset[TDataset], test: tdata.Subset[TDataset], valid: tdata.Subset[TDataset]):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.valid = valid\n",
    "    \n",
    "    def get_dataloaders(self, batch_size: int = None, shuffle: bool = None, drop_last: bool = False) -> tuple[tdata.DataLoader, tdata.DataLoader, tdata.DataLoader]:\n",
    "        return (tdata.DataLoader(self.train, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last), \\\n",
    "                tdata.DataLoader(self.test, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last), \\\n",
    "                tdata.DataLoader(self.valid, shuffle=shuffle, batch_size=batch_size, drop_last=drop_last))\n",
    "\n",
    "\n",
    "def train_split_dataset(dataset: TDataset, train_percent: float = 0.7, test_percent: float = 0.2, validate_percent: float = 0.1, random_state: int = None) -> TrainDatasetSplit[TDataset]:\n",
    "    assert math.isclose(train_percent + test_percent + validate_percent, 1.0), \"Expected train_percent + test_percent + validate_percent = 1.0!\"\n",
    "    dataset_indices = list(range(len(dataset)))\n",
    "    test_train_indices, valid_indicies = skms.train_test_split(dataset_indices, test_size=validate_percent, random_state=random_state)\n",
    "    train_indicies, test_indicies = skms.train_test_split(test_train_indices, test_size=test_percent/(train_percent + test_percent), random_state=random_state)\n",
    "    valid = tdata.Subset(dataset, valid_indicies)\n",
    "    train = tdata.Subset(dataset, train_indicies)\n",
    "    test = tdata.Subset(dataset, test_indicies)\n",
    "    split = TrainDatasetSplit(train=train, test=test, valid=valid)\n",
    "    return split\n",
    "\n",
    "\n",
    "def get_newest_model_file(prefix: str) -> str:\n",
    "    idx = -1\n",
    "    while os.path.exists(f\"{prefix}{idx + 1}.pt\"):\n",
    "        idx += 1\n",
    "    if idx >= 0:\n",
    "        return f\"{prefix}{idx}.pt\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def pretraining_loop(train_dataloader: tdata.DataLoader[MaskedTextDatasetItem], valid_dataloader: tdata.DataLoader[MaskedTextDatasetItem], config: TrainingConfig):\n",
    "    config.reset_optim()\n",
    "    model, optim = config.model, config.optim\n",
    "    log_data: list[dict] = []\n",
    "    init_epoch = 0\n",
    "    \n",
    "    model_dir = f\"{MODEL_DIR}/{config.name}/\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    existing_model_file = get_newest_model_file(model_dir + \"model_pretrain_\")\n",
    "    if existing_model_file:\n",
    "        # If model file exist, then try to continue off of where it left off\n",
    "        data = torch.load()\n",
    "        model.load_state_dict(data[\"model\"])\n",
    "        optim.load_state_dict(data[\"optim\"])\n",
    "        init_epoch = data[\"epoch\"]\n",
    "        log_data = data[\"log_data\"]\n",
    "        delta_loss_queue = [x[\"valid_loss\"] for x in log_data[:-config.epoch_window]]\n",
    "        delta_loss_moving_avg = np.average(delta_loss_queue)\n",
    "        print(f\"Resuming existing model at epoch: {epoch}, valid loss: {log_data[-1][\"valid_loss\"]}, delta_loss: {-delta_loss_moving_avg} <= {config.stop_delta_loss}\")\n",
    "        if len(delta_loss_queue) == config.epoch_window and -delta_loss_moving_avg <= config.stop_delta_loss:\n",
    "            # We hit the cutoff, return early\n",
    "            return\n",
    "        # Otherwise, continue training\n",
    "\n",
    "    print(f\"Pretraining start for '{config.name}'...\")\n",
    "    if config.mode == TrainingConfig.StopMode.DELTA_LOSS:\n",
    "        epoch_loop = itertools.count(start=init_epoch, step=1)\n",
    "    else:\n",
    "        epoch_loop = range(init_epoch, config.epochs)\n",
    "    delta_loss_queue: deque[float] = deque()\n",
    "    delta_loss_moving_avg: float = 0\n",
    "    prev_average_valid_loss = None\n",
    "    for epoch in epoch_loop:\n",
    "        # Training loop\n",
    "        loop = tqdm(cast(Iterator[MaskedTextDatasetItem], train_dataloader), leave=True)\n",
    "        total_train_loss: float = 0\n",
    "        model.train()\n",
    "        for data in loop:\n",
    "            # Reset gradient\n",
    "            optim.zero_grad()\n",
    "\n",
    "            outputs: MaskedLMOutput = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"], labels=data[\"labels\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Apply backward propagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Set info in tqdm progress bar\n",
    "            loop.set_description(f\"Epoch: {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            total_train_loss += loss.item()\n",
    "        average_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Average training loss: {average_train_loss}\")\n",
    "\n",
    "        # Validation loop\n",
    "        loop = tqdm(cast(Iterator[MaskedTextDatasetItem], valid_dataloader), leave=True)\n",
    "        total_valid_loss: float = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in loop:\n",
    "                outputs: MaskedLMOutput = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"], labels=data[\"labels\"])\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Set info in tqdm progress bar\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "                total_valid_loss += loss.item()\n",
    "        average_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "\n",
    "        print(f\"Average validation loss: {average_valid_loss}\")\n",
    "\n",
    "        # Update moving average of validation loss\n",
    "        if prev_average_valid_loss != None and TrainingConfig.StopMode.DELTA_LOSS:\n",
    "            loss_delta = average_valid_loss - prev_average_valid_loss\n",
    "            delta_loss_queue.append(loss_delta)\n",
    "            # Add new loss to moving average\n",
    "            delta_loss_moving_avg += loss_delta / config.epoch_window\n",
    "            if len(delta_loss_queue) > config.epoch_window:\n",
    "                # Remove oldest loss from moving average\n",
    "                delta_loss_moving_avg -= delta_loss_queue.popleft() / config.epoch_window\n",
    "        prev_average_valid_loss = average_valid_loss\n",
    "\n",
    "        # Make plot\n",
    "        log_data.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": average_train_loss,\n",
    "            \"valid_loss\": average_valid_loss\n",
    "        })\n",
    "        epochs = [x[\"epoch\"] for x in log_data]\n",
    "        for key in log_data[0]:\n",
    "            if key.startswith(\"_\") or key == \"epoch\":\n",
    "                continue\n",
    "            plt.plot(epochs, [item[key] for item in log_data])\n",
    "        plt.plot()\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict,\n",
    "            \"optim\": optim.state_dict,\n",
    "            \"log_data\": log_data,\n",
    "        }, model_dir + f\"model_pretrain_{epoch}.pt\")\n",
    "\n",
    "        # Break if our moving validation delta loss average is smaller than our stop_delta_loss\n",
    "        if config.mode == TrainingConfig.StopMode.DELTA_LOSS:\n",
    "            print(f\"Average delta loss ({config.epoch_window} epoch window): {-delta_loss_moving_avg} <= {config.stop_delta_loss}\")\n",
    "            if len(delta_loss_queue) >= config.epoch_window:\n",
    "                # Only start trying to exit once delta_loss_queue is filled, such that we have enough datapoints to\n",
    "                # calculate an average using a full window of epochs\n",
    "                #\n",
    "                # Delta loss is negative if we are decreaseing -> this is what we want\n",
    "                if -delta_loss_moving_avg <= config.stop_delta_loss:\n",
    "                    break\n",
    "\n",
    "\n",
    "def finetuning_loop(train_dataloader: tdata.DataLoader[ReviewsDatasetItem], valid_dataloader: tdata.DataLoader[ReviewsDatasetItem], config: TrainingConfig):\n",
    "    config.reset_optim()\n",
    "    bert_copy = copy.deepcopy(config.model.bert)\n",
    "    model = BERTSentimentClassifier(bert_copy)\n",
    "    optim = config.optim\n",
    "    init_epoch = 0\n",
    "    log_data: list[dict] = []\n",
    "    \n",
    "    model_dir = f\"{MODEL_DIR}/{config.name}/\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    existing_model_file = get_newest_model_file(model_dir + \"model_finetune_\")\n",
    "    if existing_model_file:\n",
    "        # If model file exist, then try to continue off of where it left off\n",
    "        data = torch.load()\n",
    "        model.load_state_dict(data[\"model\"])\n",
    "        optim.load_state_dict(data[\"optim\"])\n",
    "        init_epoch = data[\"epoch\"]\n",
    "        log_data = data[\"log_data\"]\n",
    "        delta_loss_queue = [x[\"valid_loss\"] for x in log_data[:-config.epoch_window]]\n",
    "        delta_loss_moving_avg = np.average(delta_loss_queue)\n",
    "        print(f\"Resuming existing model at epoch: {epoch}, valid loss: {log_data[-1][\"valid_loss\"]}, delta_loss: {-delta_loss_moving_avg} <= {config.stop_delta_loss}\")\n",
    "        if len(delta_loss_queue) == config.epoch_window and -delta_loss_moving_avg <= config.stop_delta_loss:\n",
    "            # We hit the cutoff, return early\n",
    "            return\n",
    "        # Otherwise, continue training\n",
    "    \n",
    "    print(f\"Finetuning start for '{config.name}'...\")\n",
    "    if config.mode == TrainingConfig.StopMode.DELTA_LOSS:\n",
    "        epoch_loop = itertools.count(start=init_epoch, step=1)\n",
    "    else:\n",
    "        epoch_loop = range(init_epoch, config.epochs)\n",
    "    delta_loss_queue: deque[float] = deque()\n",
    "    delta_loss_moving_avg: float = 0\n",
    "    prev_average_valid_loss = None\n",
    "    for epoch in epoch_loop:\n",
    "        # Training loop\n",
    "        loop = tqdm(cast(Iterator[ReviewsDatasetItem], train_dataloader), leave=True)\n",
    "        model.train()\n",
    "        total_train_loss: float = 0\n",
    "        for data in loop:\n",
    "            # Reset gradient\n",
    "            optim.zero_grad()\n",
    "\n",
    "            output = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"], labels=data[\"labels\"])\n",
    "            loss = output[\"loss\"]\n",
    "\n",
    "            # Apply backward propagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Set info in tqdm progress bar\n",
    "            loop.set_description(f\"Epoch: {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            total_train_loss += loss.item()\n",
    "        average_train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        print(f\"Average training loss: {average_train_loss}\")\n",
    "\n",
    "        # Validation loop\n",
    "        loop = tqdm(cast(Iterator[ReviewsDatasetItem], valid_dataloader), leave=True)\n",
    "        total_valid_loss: float = 0\n",
    "        total_correct: int = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in loop:\n",
    "                output = model(data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n",
    "                norm_ratings = output[\"norm_rating\"]\n",
    "                max_score = output[\"max_score\"]\n",
    "                pred_rating = bucketize_norm_ratings(norm_ratings, max_score)\n",
    "                correct_preds = torch.count_nonzero(pred_rating == max_score)\n",
    "                total_correct += correct_preds\n",
    "                \n",
    "                loss = output[\"loss\"]\n",
    "\n",
    "                # Set info in tqdm progress bar\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "                total_valid_loss += loss.item()\n",
    "        average_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "        accuracy = total_correct / len(valid_dataloader.dataset)\n",
    "\n",
    "        print(f\"Average validation loss: {average_valid_loss}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Update moving average of validation loss\n",
    "        if prev_average_valid_loss != None and TrainingConfig.StopMode.DELTA_LOSS:\n",
    "            loss_delta = average_valid_loss - prev_average_valid_loss\n",
    "            delta_loss_queue.append(loss_delta)\n",
    "            # Add new loss to moving average\n",
    "            delta_loss_moving_avg += loss_delta / config.epoch_window\n",
    "            if len(delta_loss_queue) > config.epoch_window:\n",
    "                # Remove oldest loss from moving average\n",
    "                delta_loss_moving_avg -= delta_loss_queue.popleft() / config.epoch_window\n",
    "        prev_average_valid_loss = average_valid_loss\n",
    "\n",
    "        # Make plot\n",
    "        log_data.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": average_train_loss,\n",
    "            \"valid_loss\": average_valid_loss,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        epochs = [x[\"epoch\"] for x in log_data]\n",
    "        for key in log_data[0]:\n",
    "            if key.startswith(\"_\") or key == \"epoch\":\n",
    "                continue\n",
    "            plt.plot(epochs, [item[key] for item in log_data])\n",
    "        plt.plot()\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict,\n",
    "            \"optim\": optim.state_dict,\n",
    "            \"log_data\": log_data,\n",
    "        }, model_dir + f\"model_finetune_{epoch}.pt\")\n",
    "\n",
    "        # Break if our moving validation delta loss average is smaller than our stop_delta_loss\n",
    "        if config.mode == TrainingConfig.StopMode.DELTA_LOSS:\n",
    "            print(f\"Average delta loss ({config.epoch_window} epoch window): {-delta_loss_moving_avg} <= {config.stop_delta_loss}\")\n",
    "            if len(delta_loss_queue) >= config.epoch_window:\n",
    "                # Only start trying to exit once delta_loss_queue is filled, such that we have enough datapoints to\n",
    "                # calculate an average using a full window of epochs\n",
    "                #\n",
    "                # Delta loss is negative if we are decreaseing -> this is what we want\n",
    "                if -delta_loss_moving_avg <= config.stop_delta_loss:\n",
    "                    break\n",
    "\n",
    "    config.model.finetuned_classifier_model = model\n",
    "\n",
    "\n",
    "def dataset_train_loop(dataset_name: str = \"myanimelist\") -> Callable[[], None]:\n",
    "    review_texts = read_review_texts(f\"{dataset_name}_reviews.csv\")\n",
    "    display(review_texts[:10])\n",
    "    \n",
    "    config = build_training_config(name=dataset_name, mode=TrainingConfig.StopMode.DELTA_LOSS, stop_delta_loss=0.05, epoch_window=5)\n",
    "    display(config)\n",
    "\n",
    "    masked_dataset = get_masked_dataset(config.tokenizer, review_texts, dataset_file=f\"{dataset_name}_masked_data.dt\")\n",
    "    masked_dataset.to(device)\n",
    "\n",
    "    split = train_split_dataset(masked_dataset, train_percent=0.7, test_percent=0.2, validate_percent=0.1)\n",
    "    train_dataloader, test_dataloader, valid_dataloader = split.get_dataloaders(batch_size=32, shuffle=True)\n",
    "    print(f\"train_dataloader: ({len(train_dataloader)} batches)\\ntest_dataloader: ({len(test_dataloader)} batches)\\nvalid_dataloader: ({len(valid_dataloader)} batches)\")\n",
    "    display(next(iter(train_dataloader)))\n",
    "\n",
    "    pretraining_loop(train_dataloader, valid_dataloader, config)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyAnimeList\n",
    "\n",
    "Training BERT on MyAnimeList only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_loop(\"myanimelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steam\n",
    "\n",
    "Training BERT on Steam only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Amazing game. Easily 30-40 hours of game play. I hope the dev continues to add more depth in the future! I'd love to see,- More zones, cities, villages etc- Additional starting scenarios (in debt, being a dealer for someone else, certain supplies/drugs not available)- Setups (undercover cops/narcs)- Raids/counter-raids (Police/cartel)- Thief's stealing supplies (if door left open, or employees turning against you)- Mixing drugs with other drugs (weed w/ cocaine)- Turf wars w/ opponent drug-lord organisation charts that you can war with, slowly discover their hierarchy and order hits or do them yourself- High volume deals (shipping supplies off to other regions)\",\n",
       " 'For a just \"released in early access game\" the devs managed to make a game that works, have a fun gameplay loop and satisfying pacing in growth. I would like to see:- a search function in the product management app- a way to change the strain\\'s name after the fact, not only when you \\'discover\\' it. - some one or faction being an obstacle. My RV gets blown to kingdom come with a dire warning, and I just go on to become a king-pin without hesitation? - I\\'d like to see the Popo chase NPCs too. There is this very important hardcore curfew! for... the PC, and no one else. - More interactions. I liked getting the quests and tasks with a little bit of story, but once you hit a certain point around the time you\\'re ready to buy your first piece of real estate that isnt renting the motel or the space above the restaurant, it\\'s lacking. Devs, Good show, and good luck! At this point, I have already gotten my money\\'s worth already, but I hope to see more. Mushrooms are fascincating, arent they?',\n",
       " 'day time comes: pickpocket the whole city night time comes: go gambling with the boys (drugs sold 0) 10/10 game',\n",
       " 'Made a type of weed that the game called Tokyo Shart that gave the smoker pitch black skin and makes them bald.',\n",
       " \"A woman came up to me and asked for weed, didn't like the price I offered, proceeded to stab me with a broken glass bottle. 10/10\",\n",
       " 'having lots of fun so far, this game NEEDS steam workshop. This getting modded would be actually amazing. Good quality and fun gameplay cycle',\n",
       " \"At first it seems like just a funny game, then 4 hours fly by and you realize it's a great game.\",\n",
       " '‚úÖ Teaches valuable life skills (supply & demand, customer service, avoiding snitches)‚úÖ Realistic police AI (they always show up when I‚Äôm finally making money)‚úÖ My friends are now suspicious of why I know so much about \"weight conversions\"‚ùå No option to escape to South America when things go bad',\n",
       " 'Look.. I\\'m old, started gaming in the 90\\'s old. This is the easiest \"trust me bro\" of my life : Buy this game.First time I buy a game and feel like I\\'m the one who robbed the devs.The game keeps blowing your mind as you progress with its layers of depth, constantly challenging you how to grow your empire and engaging you to reach that Scarface status.You are producing all of these crazy drugs causing the NPC\\'s to get addicted but none is getting more addicted than you with this game.Buy and enjoyNo thanks needed',\n",
       " \"Game is literally the peak of what we've wanted from a drug dealing game and it's Early Access! Then some losers with their mid games try to sue this one for being good because they don't know how to make their own game good. 100% support this game!\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train_loop(\"steam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacritic\n",
    "\n",
    "Training BERT on Metacritic only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Boring game that was soulless even in the beginning. Its contemporaries Roadblocks (why is this censored lol) and Blockland do everything better. Easily the most overrated game of all time.',\n",
       " 'the best, the only, the unique game in its genre.\\nThis game gave me my childhood, THIS IS THE BEST GAME',\n",
       " 'Not my cup of tea due to so many choices. Still pretty great. Memorable style of world. Diverse & expansive gameplay. Major impact on the industry.',\n",
       " 'Film He was Very average More I gave Credit By the References And also For the Nostalgia The jokes They are not Funny One Except Put Some well Specific More on al He was Good More or less',\n",
       " 'Minecraft √© muito criativo, d√° muita liberdade para os jogadores, possui um vasto lore pr√≥prio, muitos detalhes, investimento e desenvolvimento cont√≠nuo ao longo de anos ap√≥s o lan√ßamento, meus amigo sempre gostaram, sempre foi muito popular no youtube e na twitch...\\n\\nE mesmo assim eu nunca gostei tanto. Talvez pelo g√™nero de sobreviv√™ncia que n√£o √© muito a minha praia.',\n",
       " 'i just loved crushing loaf too much from the movie so of course im a absolute steve glazer. \"THESE GUYS ARE THE VILLAGERS, THEY LOOOOOOOOVE CRUSHING LOAF\"',\n",
       " 'Spent 30$ on it to play with my friends on a world, they had already been playing it so I was kind of behind ended up dying in the first 30 minutes to a glitch ruined my week',\n",
       " '–õ—É—á—à–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –≤—Å–µ—Ö –≤—Ä–µ–º—ë–Ω, –ª—É—á—à–∏–π —Å–∞—É–Ω–¥—Ç—Ä–µ–∫, –¥–∞ –∏ –ø—Ä–æ—Å—Ç–æ —Å–ø–∞—Å–∏–±–æ –∑–∞ –¥–µ—Ç—Å—Ç–≤–æ.',\n",
       " \"I've finally watched the movie and I loved it. (Sorry for the haters)\\nOf course there were some problems in it, like the zombies that are moving like a human was in a costume. But despite of that, the movie is great. \\n\\nI won't say it is the movie of the year but it deserves to be watched...\",\n",
       " \"Minecraft 2025 is a colossal misfire that makes you wonder how anyone could fumble a property as endlessly creative as Minecraft this badly especially when Super Mario proved video game adaptations can be vibrant, fun, and worth watching. Instead of a blocky wonderland bursting with imagination, we get a soulless, corporate cash-grab that's as uninspired as a dirt hut built by a first-day player. And at the center of this mess is Jack Black, whose over-the-top shtick as Steve feels less like a character and more like a bloated, annoying caricature of himself phoning it in with the energy **** who ate too many pork chops and forgot what made his Bowser a good character.Where Super Mario delivered a tight, colorful romp with a villain you couldn't help but like to hate, Minecraft stumbles through a plot so thin it could've been scribbled on a 3x5 card with a fat sharpie. The live action gimmick is a disaster with corpulent Jack Black waddling around in a turquoise shirt against a green screen looks less like an adventure and more like a sad clown act at a kid's party no one showed up to. His constant mugging and forced musical numbers grate on the nerves, turning Steve into a flabby, loud distraction instead of the everyman hero Minecraft fans **** visuals? A garish mishmash of CGI and real world actors that clash worse than a creeper in a flower biome. The charm of Minecraft's pixelated freedom is buried under a generic 'save the world' story that feels like it was ripped from a dozen better films none of which bothered to star a Jack Black who seems to think yelling and flailing is a substitute for depth. Super Mario knew how to balance nostalgia with heart; this just shovels Easter eggs and tired gags into a pit and calls it a day. It's not just terrible it's a betrayal of everything that makes Minecraft special, leaving you longing for the Mushroom Kingdom instead of this blockheaded bore.TLDR - Jack Black ruined this movie and he is a bloated slob who phoned it in.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_loop(\"metacritic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes\n",
    "\n",
    "Training BERT on Rotten Tomatoes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Una obra de arte dirigida por Denis Villeneuve. Esta pel√≠cula es una de las mejores que he visto en este g√©nero. Sin necesidad de leer los libros, comprender√°s a la perfecci√≥n todo el contexto. El director busca deleitarnos con sus tomas cinematogr√°ficas y su espectacular sonido ambiental. Una obra de arte digna de un Oscar. El √∫nico problema que le veo, y por el que no le doy un 10, es el comienzo. Puede ser muy tedioso, pero se entiende perfectamente por qu√©.\\n\\nConclusi√≥n: Esta es una pel√≠cula que definitivamente tienes que ver en alg√∫n momento. El comienzo puede ser lento porque te da mucho contexto. No pierdas la oportunidad de verla.\\n\\nCalificaci√≥n: 8.5/10',\n",
       " 'Took me a while to get into it, but as someone who has not read the Dune books, it truly feels like a book. I can see the influence and see how the book must have been like, I definitely plan to read the books at some point. Some of the decisions are a bit sub-par though. They should 100% have had a different cast. Yes, sure, they chose some good actors, but starring Zendaya in something like this is an absolute no. And I LOVE David Batista, but all I could think any time he appeared on screen, is Oh Look David Batista. You have to cast less known actors in big fantasy movies like this, because it just takes the watcher out of the movie, it takes you OUT of the beautiful world building when Zendaya with weird eyes appears on screen every so often. Completely jolts you out of the fantasy and back to \"Oh. Zendaya looks kinda weird in this movie.\" or \"haha look its David Batista\".',\n",
       " 'This might be my favorite movie !\\nDune (2021) is a sci-fi movie adapted from a roman directed by Dennis Villeneuve with Timoth√©e chalamet as Paul Atr√©ides  and Zendaya as Chani\\n\\nIn Dune(2021) Paul Atr√©ides follows his very important and powerful family to the desertic planet of Arrakis gifted by the Imperium , where a very important ressource is collected and put on the market, but after an conspiracy and a betrayal Paul have to flee on the planet where he will met the native d√©sert‚Äôs people, the Fremen\\n\\nThe actors were chosen perfectly and respected the physical descriptions given in the roman with the exception of some like Liet Kynes who was played by a woman. I also loved the makeup art on the Harkonnen family and the Eyes of Ibad\\n\\nThe film technique were amazing ! I think we cannot represent Arrakis better than they did , the lighting were very good on the desert and the pictures and ambiance were perfect \\nThe soundtrack by Hans Zimmer was flawless !\\nI really loved the costumes, from the Paul outfit on Caladan or Jessica outfit on the Arrakis arrival to the distille and Harkonnen Armor\\nAnd that‚Äôs why this movie won 6 oscars for 10 nominations\\n\\nDune explore theme like ecology power and colonialism.The film shows how the struggle for the rare ressource spice touch political, economic, and cultural conflicts, mirroring real-world tensions over oil and natural resources.  And my favorite theme is religion because its a big interest to me at this moment in my life, i love debatting about religion, learning about the story of religion so i absolutly loved the religion th√®me and everything its brings to the story \\n\\nFinally i really recommend to the movie watcher to try to read dune !!! Even though the adaptation is perfect in therm of cinematography i think the plot is very hard to fully understand and the movie miss a lot of scene and aspect of the story . So as a lover of Dune universe i fully recommend the book before watching the movie !',\n",
       " 'Extremely slow & boring, most action occurs in the dark and is hard to see. I was 2 hours into it before it grabbed my interest then was very anticlimactic.',\n",
       " 'Exceptional, wildly fantastic and never a dull moment. Truly spectacular in every possible way a movie can be. Cast all excels in character, cinematography out of this world. Worth the wait, for sure.',\n",
       " 'EPIC SCALE MOVIE\\nCGI WAS AMAZING, as well as the score, acting, plot, and cinematography.\\nthe movie overstayed its welcome a little bit, with some scenes dragging out the film to its colossal 2:30 runtime. this being said, the movie was great and i would definitely recommend to any drama or sci fi lovers.',\n",
       " 'Um espet√°culo. Na minha opini√£o um dos melhores filmes dos √∫ltimos anos.',\n",
       " \"I've watched üëÄ this movie at least ten times, that should say Everything!!!\",\n",
       " 'Not bad. I still prefer the classic one from 1984 though (which I\\'ve just rewatched). I can\\'t make comparisons with the books because I didn\\'t read them, so I don\\'t know how much the films differ from the original story. I can only make a comparison with the first film and I definitely liked more the characters of that one. I\\'ve found them having more personality. In this one I find the characters more \"flat\". It\\'s still not bad though, good special effects and interesting lore',\n",
       " 'A slogfest of a story, without any exciting moments to consider this movie memorable. The 1.5 stars are for the fact that the movie is undeniably beautiful with incredible sound. Feels like this movie was made to set up for the vastly superior second chapter.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_loop(\"rotten_tomatoes\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
